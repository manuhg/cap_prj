\chapter{Related Work}

This chapter presents an overview of existing technologies and research efforts relevant to the development of efficient, on-device language model-based applications. It focuses on three major areas: Retrieval-Augmented Generation (RAG), efficient LLM runtimes like \texttt{llama.cpp}, and lightweight distribution and serving solutions such as Ollama and Llamafile. It also looks at the work done by tinygrad project in attempting to reverse engineer the NPU API.


%----------------------
\section{Retrieval-Augmented Generation (RAG)}
\label{sec:RAG}
%----------------------

Retrieval-Augmented Generation (RAG) is an architectural paradigm that enhances the factual accuracy and contextual relevance of large language models (LLMs) by incorporating external knowledge retrieval into the generation process. Unlike traditional LLMs that rely solely on internal model weights, RAG allows the model to fetch and condition its output on relevant documents retrieved from a corpus \cite{lewis2021retrieval}.

A typical RAG pipeline consists of the following phases:

\begin{enumerate}
    \item \textbf{Query Formulation:} The input query from the user is either used directly or rephrased using prompt engineering techniques to improve retrieval relevance.
    \item \textbf{Retrieval:} A vector store or dense retriever (e.g., using FAISS, Milvus, or ElasticSearch) is queried to obtain top-$k$ semantically relevant documents based on vector similarity.
    \item \textbf{Context Injection:} Retrieved documents are concatenated with the original query and formatted into a prompt.
    \item \textbf{Generation:} The formatted prompt is passed to the language model to generate a context-aware and informative response.
\end{enumerate}

RAG has become a cornerstone for building knowledge-intensive NLP systems, especially in enterprise search, question answering, and summarization tasks \cite{rag_blog, langchain_rag}.


%----------------------
\section{llama.cpp}
\label{sec:llama_cpp}
%----------------------

\texttt{llama.cpp} is a C++ implementation of LLaMA models developed by Meta, optimized for local inference on commodity hardware without GPU requirements. Built upon the GGML tensor library, it provides quantized inference for large models using CPU-friendly formats like 4-bit and 5-bit quantization, making it suitable for running models such as LLaMA, Mistral, and other open-weight transformers on devices ranging from laptops to Raspberry Pi \cite{llamacpp}.

Key features of \texttt{llama.cpp} include:
\begin{itemize}
    \item Highly efficient CPU inference with quantized models.
    \item Cross-platform support (macOS, Linux, Windows).
    \item Integration with popular tooling such as LangChain and Open Interpreter.
    \item Support for multi-threaded inference and memory-mapped model weights for efficient memory usage.
\end{itemize}

This project is a foundation for many desktop LLM applications that require local execution and privacy-aware computation.


%----------------------
\section{Ollama}
\label{sec:ollama}
%----------------------

Ollama is a developer-friendly platform for running LLMs locally with simplified model management and serving. It wraps models like LLaMA 2, Mistral, and Code LLaMA into a streamlined runtime with a CLI and RESTful API, abstracting away hardware-specific setup and providing a plug-and-play experience for developers \cite{ollama}.

Ollama supports:
\begin{itemize}
    \item Running quantized models locally with GPU acceleration where available.
    \item Seamless model downloading and serving.
    \item Custom model creation using a simple Modelfile syntax.
\end{itemize}

It is widely used for prototyping private, offline chatbots and assistants. However, the users need to be technically savvy in cases of issues downloading or running the models. Furthermore, models often may not be quantized and could lead to downloading of model weights in order of many GBs.


%----------------------
\section{Llamafile}
\label{sec:llamafile}
%----------------------


Llamafile, developed by Mozilla-Ocho, enables packaging a complete LLM runtime into a single, self-contained executable file \cite{llamafile}. It leverages the \texttt{llama.cpp} backend and Cosmopolitan Libc to build universal binaries that run across major operating systems (Windows, macOS, Linux) without requiring dependencies.

Notable features:
\begin{itemize}
    \item Distributable as a single file under 1GB (depending on model).
    \item Useful for shipping LLM-based tools with zero-install requirements.
    \item Integrates with web frontends for local chatbot deployment.
\end{itemize}

Llamafile is widely used for prototyping private, offline chatbots and assistants. However, it often requires users to be technically proficient, especially when encountering issues related to downloading or executing models. Moreover, many models are not pre-quantized, potentially resulting in downloads of several gigabytes of model weights.
%----------------------
\section{Tinygrad project and Apple Neural Engine (ANE)}
\label{sec:ANEAPI}
%----------------------

The Apple Neural Engine (ANE) is a custom neural processing unit designed by Apple to accelerate machine learning workloads on its silicon platforms. Introduced with the A11 Bionic chip, the ANE has evolved into a high-performance, low-power DMA-based inference engine embedded in Apple's M-series chips. This section synthesizes insights obtained by the reverse-engineering efforts of the \texttt{tinygrad}~\cite{tinygrad2023ane} project to examine ANE's architecture, capabilities, and compilation flow.

\subsection{Hardware Overview}

The ANE operates primarily as a DMA engine optimized for convolutional operations and supports a wide range of neural network layers and fused operations. Its key hardware features include:

\begin{itemize}
  \item \textbf{16-core architecture:} A 16-wide Kernel DMA engine for parallel computation.
  \item \textbf{5D Tensor Support:} Tensors are structured with width (column), height (row), planes (channels), depth, and group (batch).
  \item \textbf{Supported Data Types:} \texttt{UInt8}, \texttt{Int8}, and \texttt{Float16} (with \texttt{Float32} inputs automatically downcast).
  \item \textbf{Manually Managed 4MB L2 Cache:} Applied only to input/output data; weights are embedded in the compiled program.
  \item \textbf{Execution Unit:} Executes up to 0x300 micro-operations per instruction.
  \item \textbf{Performance:} Approximate 11 TOPS throughput, assuming 32\texttimes32 MAC at ~335 MHz.
\end{itemize}

All memory strides are constrained to multiples of 0x40 bytes, reflecting hardware alignment requirements.

\subsection{Software and Compilation Stack}

The ANE software stack is heavily abstracted behind Apple's proprietary frameworks but has been reverse-engineered to reveal a structured flow (as shown in Fig~\ref{fig:ane-workflow}):
\begin{figure}[h]
    \centering
    \includegraphics[width=0.25\linewidth]{images/ane-workflow.jpg}
    \caption{Apple Neural Engine Workflow}
    \label{fig:ane-workflow}
\end{figure}

\begin{enumerate}
  \item \textbf{Model Definition:} Models are authored in Keras or ONNX.
  \item \textbf{Conversion:} Models are converted to CoreML format using open-source tools such as \texttt{coremltools}.
  \item \textbf{Intermediate Representation:} CoreML is internally converted into \texttt{net.plist} by Apple's Espresso framework.
  \item \textbf{Compilation:} The \texttt{ANECompiler} service transforms \texttt{net.plist} into a hardware-specific binary (.\texttt{hwx}), a Mach-O formatted executable.
  \item \textbf{Execution:} The \texttt{AppleNeuralEngine} and \texttt{ANEServices} handle execution via the kernel extension \texttt{AppleH11ANEInterface}.
\end{enumerate}


\subsection{Instruction Format and Operation Structure}

Each ANE instruction is 0x300 bytes and comprises multiple segments:

\begin{itemize}
  \item \textbf{Header:} Includes DMA addresses and next-op offset.
  \item \textbf{KernelDMASrc:} Specifies weights, bias, and channel usage.
  \item \textbf{Common:} Describes input/output shapes, types, kernel size, and padding.
  \item \textbf{TileDMASrc/TDMADst:} Layout and stride configurations for input/output tensors.
  \item \textbf{L2 and NE:} L2 cache flags and activation parameters.
\end{itemize}

\subsection{Supported Operations and Activations}

ANE supports a variety of operations:

\begin{itemize}
  \item \textbf{Core Ops:} CONV, POOL, EW, CONCAT, RESHAPE, MATRIX\_MULT, TRANSPOSE
  \item \textbf{Advanced:} SCALE\_BIAS, SOFTMAX, INSTANCE\_NORM, BROADCAST, L2\_NORM
  \item \textbf{Fused Ops:} NEFUSED\_CONV, PEFUSED\_POOL, etc.
  \item \textbf{Activations:} RELU, SIGMOID, TANH, CLAMPED\_RELU, PRELU, LOG2/EXP2, CUSTOM\_LUT
\end{itemize}

Over 30 activation functions are supported in hardware.

\subsection{tinygrad Implementation}

The \texttt{tinygrad} project interfaces directly with ANE through a three-stage pipeline:

\begin{itemize}
  \item \textbf{1\_build:} Generates CoreML models using \texttt{coremltools}.
  \item \textbf{2\_compile:} Uses Objective-C and Apple's private ANECompiler framework to compile models into HWX binaries.
  \item \textbf{3\_run:} Loads HWX binaries and executes them on ANE using custom Objective-C wrappers around \texttt{AppleH11ANEInterface}.
\end{itemize}

The implementation also includes tools like \texttt{hwx\_parse.py} for disassembling HWX files and visualizing internal ops.

\subsection{Security and Access}

Execution on ANE requires system entitlements that are typically unavailable to third-party applications:

\begin{itemize}
  \item \texttt{com.apple.ane.iokit-user-access}
  \item Workarounds: amfid patching, kernel extension modification, or use of provisioning profiles.
\end{itemize}

\subsection{Conclusion}

The ANE represents a proprietary, highly optimized inference accelerator that is difficult to access and understand due to Appleâ€™s closed ecosystem. Reverse engineering, as demonstrated by \texttt{tinygrad}, reveals a modular, DMA-centric architecture capable of executing complex neural network operations at high throughput and low latency. As Apple continues to iterate on the ANE, deeper access and tooling may unlock broader ML deployment options on Apple hardware.
