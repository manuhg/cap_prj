@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  pages={5998--6008},
  year={2017}
}

@misc{llama_server,
  author       = {ggml-org},
  title        = {llama.cpp - Server Example},
  year         = {2024},
  note         = {Accessed 2025-05-26},
  howpublished = {\url{https://github.com/ggml-org/llama.cpp/tree/master/tools/server}}
}

@misc{llama_simple,
  author       = {ggml-org},
  title        = {llama.cpp - Simple Example},
  year         = {2024},
  note         = {Accessed 2025-05-26},
  howpublished = {\url{https://github.com/ggml-org/llama.cpp/tree/master/examples/simple}}
}

@misc{llama_embedding,
  author       = {ggml-org},
  title        = {llama.cpp - Embedding Example},
  year         = {2024},
  note         = {Accessed 2025-05-26},
  howpublished = {\url{https://github.com/ggml-org/llama.cpp/tree/master/examples/embedding}}
}

@misc{langchain2023textsplitters,
  author       = {LangChain},
  title        = {Text Splitters},
  year         = {2023},
  howpublished = {\url{https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitter/}},
  note         = {Accessed May 2025}
}

@online{apple2024ane,
  author = {{Apple Inc.}},
  title = {Apple Neural Engine (ANE) Performance Across Devices},
  year = {2024},
  url = {https://www.makeuseof.com/what-is-a-neural-engine-how-does-it-work/},
  note = {Accessed May 13, 2025. Reports ANE performance ranging from 0.6 TOPS (A11) to 38 TOPS (M4).}
}

@online{tinygrad2023ane,
  author       = {Tinygrad},
  title        = {Apple Neural Engine Reverse Engineered for C++},
  year         = {2023},
  howpublished = {\url{https://github.com/geohot/tinygrad/tree/master/accel/ane}},
  note         = {GitHub repository documenting reverse engineering of Apple's ANE}
}

@online{chatgpt2022,
  author = {{OpenAI}},
  title = {{ChatGPT}},
  year = {2022},
  howpublished = {\url{https://chat.openai.com}},
  note = {LLM with a chat interface, launched in November 2022}
}


@online{ollama2023,
  title={{Ollama}},
  year={2023},
  howpublished={\url{https://ollama.com}},
  note={Desktop application to download and run a specific LLM}
}

@online{llamafile2023,
  title={{LLamaFile}},
  year={2023},
  howpublished={\url{https://github.com/Mozilla-Ocho/llamafile}},
  note={Command-line app to package and run an LLM}
}

@inproceedings{bert-score,
  title={BERTScore: Evaluating Text Generation with BERT},
  author={Tianyi Zhang* and Varsha Kishore* and Felix Wu* and Kilian Q. Weinberger and Yoav Artzi},
  booktitle={International Conference on Learning Representations},
  year={2020},
  url={https://openreview.net/forum?id=SkeHuCVFDr}
}

@online{llamacpp2023,
  title={{LLaMa.cpp}},
  author={Gerganov, Georgi},
  year={2023},
  howpublished={\url{https://github.com/ggerganov/llama.cpp}},
  note={C++ framework to run open source LLMs on local hardware}
}

@online{kagglearxiv,
  title={{ArXiv Papers Dataset on Kaggle}},
  year={2023},
  howpublished={\url{https://www.kaggle.com/datasets/Cornell-University/arxiv}},
  note={ArXiv metadata dataset provided on Kaggle}
}
@article{devlin2018bert,
  title={BERT: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{liu2019text,
  title={Text summarization with pretrained encoders},
  author={Liu, Yang and Lapata, Mirella},
  journal={arXiv preprint arXiv:1908.08345},
  year={2019}
}

@article{beltagy2020longformer,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020}
}

@article{bommasani2021opportunities,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}

@misc{openai2023gpt4,
  title={GPT-4 Technical Report},
  author={OpenAI},
  year={2023},
  howpublished={\url{https://openai.com/research/gpt-4}},
  note={Accessed 2024}
}

@article{touvron2023llama,
  title={LLaMA: Open and Efficient Foundation Language Models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothée and Rozière, Baptiste and Goyal, Naman and Gallé, Matthias and Lample, Guillaume},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{izacard2021leveraging,
  title={Leveraging passage retrieval with generative models for open domain question answering},
  author={Izacard, Gautier and Grave, Edouard},
  journal={arXiv preprint arXiv:2007.01282},
  year={2021}
}


@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of Machine Learning Research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@article{jacob2017quantization,
  title={Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference},
  author={Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
  journal={arXiv preprint arXiv:1712.05877},
  year={2017}
}

@article{hu2021lora,
  title={LoRA: Low-Rank Adaptation of Large Language Models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}


@online{alammar2018illustrated,
  author    = {Alammar, Jay},
  title     = {The Illustrated Transformer},
  year      = {2018},
  url       = {https://jalammar.github.io/illustrated-transformer/},
  note      = {Accessed May 2025}
}

@online{huggingface_gentext,
  author    = {{Hugging Face}},
  title     = {Text Generation Strategies},
  year      = {2021},
  url       = {https://huggingface.co/blog/how-to-generate},
  note      = {Accessed May 2025}
}


@article{lewis2020rag,
  title={Retrieval-augmented generation for knowledge-intensive NLP tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Kulkarni, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}

@article{deschenes2024survey,
  title={A Survey on Student Use of Generative AI Chatbots for Academic Research},
  author={Deschenes, Amy and McMahon, Meg},
  journal={Evidence Based Library and Information Practice},
  volume={19},
  number={2},
  pages={2--22},
  year={2024},
  doi={10.18438/eblip30512}
}

@article{hosseini2023chatgpt,
  title={An exploratory survey about using ChatGPT in education, healthcare, and research},
  author={Hosseini, Mohammad and Horbach, Serge and Van den Broek, Tanja and De Bruin, Boudewijn and Wieringa, Sietse},
  journal={medRxiv},
  year={2023},
  doi={10.1101/2023.03.31.23287979}
}

@article{mattern2023membership,
  title={Membership Inference Attacks against Language Models via Neighbourhood Comparison},
  author={Mattern, Philipp and Sattler, Felix and Schmeck, Hartmut and Pfitzner, Bastian},
  journal={arXiv preprint arXiv:2306.04554},
  year={2023}
}

@article{nasr2023extraction,
  title={Scalable Extraction of Training Data from (Production) Language Models},
  author={Nasr, Milad and Carlini, Nicholas and Tramer, Florian and Shokri, Reza},
  journal={arXiv preprint arXiv:2311.17035},
  year={2023}
}

@article{zhang2020bertscore,
  title={BERTScore: Evaluating Text Generation with BERT},
  author={Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q and Artzi, Yoav},
  journal={arXiv preprint arXiv:2004.02047},
  year={2020}
}

@article{mikolov2013efficient,
  title={Efficient Estimation of Word Representations in Vector Space},
  author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  journal={arXiv preprint arXiv:1301.3781},
  year={2013}
}

@online{glenn2024mastering,
  author = {{Glenn}},
  title = {{Mastering Retrieval-Augmented Generation (RAG) Architecture}},
  year = {2024},
  howpublished = {\url{https://blog.stackademic.com/mastering-retrieval-augmented-generation-rag-architecture-unleash-the-power-of-large-language-a1d2be5f348c}},
  note = {Mastering Retrieval-Augmented Generation (RAG) Architecture}
}

@misc{langchain2023building,
  author       = {LangChain},
  title        = {Building a RAG App},
  year         = {2023},
}


@misc{touvron2024llama3,
  author       = {Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and others},
  title        = {LLaMA 3: Open Foundation and Instruction-Tuned Models},
  year         = {2024},
  howpublished = {\url{https://ai.meta.com/blog/meta-llama-3/}},
  note         = {Meta AI, Accessed May 2025}
}

@article{brown2020language,
  author       = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  title        = {Language Models are Few-Shot Learners},
  journal      = {arXiv preprint arXiv:2005.14165},
  year         = {2020},
}

@article{bashir2023context,
  author       = {Bashir},
  title        = {In-Context Learning, In Context},
  year         = {2023},
}

@misc{langchain2023text,
  author       = {LangChain},
  title        = {Text Splitters},
  year         = {2023},
}

@misc{agisphere2023context,
  author       = {AGI-Sphere},
  title        = {Context Length in LLMs},
  year         = {2023},
}

@article{liu2023long,
  author       = {Liu, Nelson F. and Shen, Tianyi and Brahman, Faeze and Diab, Mona and Smith, Noah A. and Choi, Yejin},
  title        = {Long in the Middle: How Language Models Use Long Contexts},
  journal      = {arXiv preprint arXiv:2307.03172},
  year         = {2023},
}

@article{sennrich2015neural,
  author       = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  title        = {Neural Machine Translation of Rare Words with Subword Units},
  journal      = {arXiv preprint arXiv:1508.07909},
  year         = {2015},
}


@article{sugawara2016approximately,
  author       = {Sugawara, Shinya and Kobayashi, Hayato and Iwasaki, Manabu},
  title        = {On Approximately Searching for Similar Word Embeddings},
  journal      = {arXiv preprint arXiv:1604.00417},
  year         = {2016},
}

@misc{pgvector,
  author       = {Felix L. Hsu},
  title        = {{pgvector: Vector similarity search for PostgreSQL}},
  howpublished = {\url{https://github.com/pgvector/pgvector}},
  year         = {2023},
  note         = {Accessed: 2025-05-23}
}

@misc{langchain,
  title = {LangChain: Building Applications with LLMs through Composability},
  author = {{LangChain Contributors}},
  year = {2023},
  note = {\url{https://www.langchain.com/}},
}

@misc{llamaindex,
  title = {LlamaIndex (GPT Index)},
  author = {{LlamaIndex Team}},
  year = {2023},
  note = {\url{https://www.llamaindex.ai/}},
}

@misc{haystack,
  title = {Haystack: Open Source NLP Framework for Question Answering, Summarization, and more},
  author = {{deepset AI}},
  year = {2023},
  note = {\url{https://haystack.deepset.ai/}},
}

@misc{ragas,
  title = {RAGAS: Retrieval-Augmented Generation Assessment},
  author = {{RAGAS Contributors}},
  year = {2024},
  note = {\url{https://github.com/explodinggradients/ragas}},
}

@misc{autogen,
  title = {AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Collaboration},
  author = {Microsoft Research},
  year = {2023},
  note = {\url{https://github.com/microsoft/autogen}},
}

@article{gor2023router,
  title={RouterLLM: An Expert Routing System for Cost-Efficient Inference of Large Language Models},
  author={Gor, Rapha{\"e}l and Tan, Xiao and Ouyang, Zi and Zhang, Zekun and Wei, Jinjie and He, Yiheng and Xu, Lei and Wu, Yuexin},
  journal={arXiv preprint arXiv:2309.13285},
  year={2023}
}

@misc{labelbox2023vectorsimilarity,
  author       = {Labelbox},
  title        = {Vector similarity search techniques},
  year         = {2023},
  howpublished = {\url{https://labelbox.com/blog/how-vector-similarity-search-works/}},}

@article{hinton2015distilling,
  title={Distilling the Knowledge in a Neural Network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015},
  url={https://arxiv.org/abs/1503.02531}
}

@article{wang2020minilm,
  title={MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers},
  author={Wang, Wenhui and Wei, Furu and Dong, Li and Bao, Hang and Yang, Nan and Zhou, Ming},
  journal={arXiv preprint arXiv:2002.10957},
  year={2020},
  url={https://arxiv.org/abs/2002.10957}
}
}
@article{wang2024bestpractices,
  title={Searching for Best Practices in Retrieval-Augmented Generation},
  author={Wang, Xiaohua and Wang, Zhenghua},
  journal={arXiv preprint arXiv:2407.01219},
  year={2024},
  url={https://arxiv.org/abs/2407.01219}
}

@misc{cardenas2023rag,
  author       = {Erika Cardenas and Connor Shorten},
  title        = {An Overview on RAG Evaluation},
  year         = {2023},
  url          = {https://weaviate.io/blog/rag-evaluation},
  note         = {Accessed: 2025-05-22}
}
@article{guo2020accelerating,
  author       = {Guo, Ruiqi and Sun, Philip and Lindgren, Erik and Geng, Quan and Simcha, David and Chern, Felix and Kumar, Sanjiv},
  title        = {Accelerating Large-Scale Inference with Anisotropic Vector Quantization},
  journal      = {arXiv preprint arXiv:1908.10396},
  year         = {2020},
}

@article{steck2024cosine,
  author       = {Steck, Harald and Pham, Hieu and Ding, Dawen and Thai, Lam and Yue, Hu and Han, Wei and Soar, Jeanne and Sahasrabudhe, Mihir and Shalizi, Cosma},
  title        = {Is Cosine-Similarity of Embeddings Really About Similarity?},
  journal      = {arXiv preprint arXiv:2403.05440},
  year         = {2024},
}

@misc{currentslab2023vector,
  author       = {CurrentsLab},
  title        = {Vector Search Techniques and Engines},
  year         = {2023},
}

@article{johnson2017billion,
  author       = {Johnson, Jeff and Douze, Matthijs and Jégou, Hervé},
  title        = {Billion-scale similarity search with GPUs},
  journal      = {arXiv preprint arXiv:1702.08734},
  year         = {2017},
}

@misc{nabi2024all,
  author       = {Nabi},
  title        = {All You Need To Know About LLM Text Generation},
  year         = {2024},
}

@article{cardenas2023overview,
  author       = {Cardenas},
  title        = {Overview of RAG Evaluation},
  year         = {2023},
}

@misc{hollemans2020apple,
  author       = {Hollemans},
  title        = {Apple M1 chip architecture},
  year         = {2020},
}

@techreport{apple2020m1,
  author       = {Apple Corp.},
  title        = {Apple M1 Overview},
  institution  = {Apple Inc.},
  year         = {2020},
}

@manual{apple2020metal,
  author       = {Apple Corp.},
  title        = {Apple Metal GPU Developer Guide},
  organization = {Apple Inc.},
  year         = {2020},
}

@misc{tinygrad2023apple,
  author       = {Tinygrad},
  title        = {Apple Neural Engine Reverse Engineered for C++},
  year         = {2023},
}

@misc{llamacpp2024running,
  author       = {LLaMa.cpp},
  title        = {Running LLM as a server},
  year         = {2024},
}

@article{hubara2016quantized,
  author       = {Hubara, Itay and Courbariaux, Matthieu and Soudry, Daniel and El-Yaniv, Ran and Bengio, Yoshua},
  title        = {Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations},
  journal      = {arXiv preprint arXiv:1609.07061},
  year         = {2016},
}

@misc{coreml2023max,
  author       = {Apple Inc.},
  title        = {CoreML – max() function},
  year         = {2023},
}

@article{pope2022efficiently,
  author       = {Pope, Reiner and Tamkin, Alex and Akyürek, Ekin and Dathathri, Suhas and Hernandez, Danny and Goldie, Andrew},
  title        = {Efficiently Scaling Transformer Inference},
  journal      = {arXiv preprint arXiv:2211.05102},
  year         = {2022},
}

@article{leviathan2023fast,
  author       = {Leviathan, Yaniv and Kalman, Matan and Matias, Yossi},
  title        = {Fast Inference from Transformers via Speculative Decoding},
  journal      = {arXiv preprint arXiv:2303.16207},
  year         = {2023},
}

@article{javaheripi2023phi,
  author       = {Javaheripi, Mojan and Diab, Mona and Gehrmann, Sebastian and Narayanan, Deepak and Rush, Alexander M. and Sellam, Thibault and Xu, Dacheng and Saeedi, Arash and Kaplan, Jared and Vetrov, Dmitry and others},
  title        = {Phi-2: The surprising power of small language models},
  year         = {2023},
}

@article{abdin2024phi,
  author       = {Abdin, Amr and Achiam, Joshua and Adler, Stephen and Babuschkin, Igor and Chen, Xi and Cheng, Kuan and Dorokhova, Ekaterina and Hilton, Jacob and Hofmann, Marek and Kamenev, Alex and others},
  title        = {Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone},
  journal      = {arXiv preprint arXiv:2404.14219},
  year         = {2024},
}

@techreport{google2024gemma,
  author       = {Google Gemma Team},
  title        = {Gemma: Open Models Based on Gemini Research and Technology},
  institution  = {Google},
  year         = {2024},
}

@techreport{fair2024llama,
  author       = {FAIR Team},
  title        = {Llama 3.2: Revolutionizing edge AI and vision with open, customizable models},
  institution  = {Meta AI},
  year         = {2024},
}

@article{talamdupula2024guide,
  author       = {Talamdupula},
  title        = {A Guide to Quantization in LLMs},
  year         = {2024},
}

@article{li2024quantization,
  author       = {Li},
  title        = {Quantization tech of LLMs-GGUF},
  year         = {2024},
}

@misc{apple2023converting,
  author       = {Apple Inc.},
  title        = {Apple CoreML – Converting PyTorch Model to CoreML},
  year         = {2023},
}

@misc{gerganov2020llamacpp,
  author       = {Gerganov},
  title        = {LLaMa.cpp project},
  year         = {2020},
  howpublished = {\url{https://github.com/ggml-org/llama.cpp}}
}

@misc{apple2024coremlconvert,
  author       = {{Apple CoreML}},
  title        = {CoreML – Converting PyTorch Model to CoreML},
  year         = {2024},
  howpublished = {\url{https://developer.apple.com/documentation/coreml/}},
  note         = {Accessed May 2025}
}

@article{chandradas2024security,
  author  = {S. Chandra Das and K. Roy and K. Bhawal and A. A. Mahmud and S. M. H. Khan and S. Alam},
  title   = {Security and Privacy Challenges of Large Language Models},
  journal = {arXiv preprint arXiv:2403.09793},
  year    = {2024},
  url     = {https://arxiv.org/abs/2403.09793}
}

@inproceedings{shokri2016membership,
  author    = {Reza Shokri and Marco Stronati and Congzheng Song and Vitaly Shmatikov},
  title     = {Membership Inference Attacks Against Machine Learning Models},
  booktitle = {Proceedings of the 2017 IEEE Symposium on Security and Privacy (SP)},
  year      = {2017},
  pages     = {3--18},
  publisher = {IEEE},
  doi       = {10.1109/SP.2017.41}
}

@misc{man7_mmap,
  author       = {{man7.org}},
  title        = {mmap(2) - Linux manual page},
  year         = {2024},
  howpublished = {\url{https://man7.org/linux/man-pages/man2/mmap.2.html}},
  note         = {Accessed May 14, 2025}
}

@article{lewis2021retrieval,
  author  = {Patrick Lewis and Ethan Perez and Aleksandra Piktus and Fabio Petroni and Vladimir Karpukhin and Naman Goyal and Heinrich Küttler and Mike Lewis and Wen-tau Yih and Tim Rockt{\"a}schel and Sebastian Riedel and Douwe Kiela},
  title   = {Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks},
  journal = {Transactions of the Association for Computational Linguistics},
  volume  = {9},
  pages   = {447--466},
  year    = {2021},
  doi     = {10.1162/tacl_a_00373}
}

@misc{rag_blog,
  author       = {Stackademic Blog},
  title        = {Mastering Retrieval-Augmented Generation (RAG) Architecture},
  year         = {2023},
  howpublished = {\url{https://blog.stackademic.com/mastering-retrieval-augmented-generation-rag-architecture-unleash-the-power-of-large-language-a1d2be5f348c}},
  note         = {Accessed May 2025}
}

@misc{langchain_rag,
  author       = {LangChain},
  title        = {RAG Tutorial},
  year         = {2023},
  howpublished = {\url{https://python.langchain.com/docs/tutorials/rag/}},
  note         = {Accessed May 2025}
}
@misc{llamacpp,
  author       = {ggml-org},
  title        = {llama.cpp},
  year         = {2023},
  howpublished = {\url{https://github.com/ggml-org/llama.cpp}},
  note         = {GitHub repository. Accessed May 2025}
}

@misc{ollama,
  author       = {Ollama},
  title        = {Ollama},
  year         = {2023},
  howpublished = {\url{https://ollama.com/}},
  note         = {Accessed May 2025}
}

@misc{llamafile,
  author       = {Mozilla-Ocho},
  title        = {Llamafile},
  year         = {2023},
  howpublished = {\url{https://github.com/Mozilla-Ocho/llamafile}},
  note         = {Accessed May 2025}
}