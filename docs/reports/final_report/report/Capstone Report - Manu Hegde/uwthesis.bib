@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  pages={5998--6008},
  year={2017}
}

@article{devlin2018bert,
  title={BERT: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{liu2019text,
  title={Text summarization with pretrained encoders},
  author={Liu, Yang and Lapata, Mirella},
  journal={arXiv preprint arXiv:1908.08345},
  year={2019}
}

@article{beltagy2020longformer,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020}
}

@article{bommasani2021opportunities,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}

@misc{openai2023gpt4,
  title={GPT-4 Technical Report},
  author={OpenAI},
  year={2023},
  howpublished={\url{https://openai.com/research/gpt-4}},
  note={Accessed 2024}
}

@article{touvron2023llama,
  title={LLaMA: Open and Efficient Foundation Language Models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothée and Rozière, Baptiste and Goyal, Naman and Gallé, Matthias and Lample, Guillaume},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{izacard2021leveraging,
  title={Leveraging passage retrieval with generative models for open domain question answering},
  author={Izacard, Gautier and Grave, Edouard},
  journal={arXiv preprint arXiv:2007.01282},
  year={2021}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of Machine Learning Research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@article{jacob2017quantization,
  title={Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference},
  author={Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
  journal={arXiv preprint arXiv:1712.05877},
  year={2017}
}

@article{hu2021lora,
  title={LoRA: Low-Rank Adaptation of Large Language Models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@article{lewis2020rag,
  title={Retrieval-augmented generation for knowledge-intensive NLP tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Kulkarni, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}

@article{deschenes2024survey,
  title={A Survey on Student Use of Generative AI Chatbots for Academic Research},
  author={Deschenes, Amy and McMahon, Meg},
  journal={Evidence Based Library and Information Practice},
  volume={19},
  number={2},
  pages={2--22},
  year={2024},
  doi={10.18438/eblip30512}
}

@article{hosseini2023chatgpt,
  title={An exploratory survey about using ChatGPT in education, healthcare, and research},
  author={Hosseini, Mohammad and Horbach, Serge and Van den Broek, Tanja and De Bruin, Boudewijn and Wieringa, Sietse},
  journal={medRxiv},
  year={2023},
  doi={10.1101/2023.03.31.23287979}
}

@article{mattern2023membership,
  title={Membership Inference Attacks against Language Models via Neighbourhood Comparison},
  author={Mattern, Philipp and Sattler, Felix and Schmeck, Hartmut and Pfitzner, Bastian},
  journal={arXiv preprint arXiv:2306.04554},
  year={2023}
}

@article{nasr2023extraction,
  title={Scalable Extraction of Training Data from (Production) Language Models},
  author={Nasr, Milad and Carlini, Nicholas and Tramer, Florian and Shokri, Reza},
  journal={arXiv preprint arXiv:2311.17035},
  year={2023}
}

@article{zhang2020bertscore,
  title={BERTScore: Evaluating Text Generation with BERT},
  author={Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q and Artzi, Yoav},
  journal={arXiv preprint arXiv:2004.02047},
  year={2020}
}

@article{mikolov2013efficient,
  title={Efficient Estimation of Word Representations in Vector Space},
  author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  journal={arXiv preprint arXiv:1301.3781},
  year={2013}
}

