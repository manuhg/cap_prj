\chapter{Related Work}

This chapter presents an overview of existing technologies and research efforts relevant to the development of efficient, on-device language model-based applications. It focuses on three major areas: Retrieval-Augmented Generation (RAG), efficient LLM runtimes like \texttt{llama.cpp}, and lightweight distribution and serving solutions such as Ollama and Llamafile.

\section{Retrieval-Augmented Generation (RAG)}

Retrieval-Augmented Generation (RAG) is an architectural paradigm that enhances the factual accuracy and contextual relevance of large language models (LLMs) by incorporating external knowledge retrieval into the generation process. Unlike traditional LLMs that rely solely on internal model weights, RAG allows the model to fetch and condition its output on relevant documents retrieved from a corpus \cite{lewis2021retrieval}.

A typical RAG pipeline consists of the following phases:

\begin{enumerate}
    \item \textbf{Query Formulation:} The input query from the user is either used directly or rephrased using prompt engineering techniques to improve retrieval relevance.
    \item \textbf{Retrieval:} A vector store or dense retriever (e.g., using FAISS, Milvus, or ElasticSearch) is queried to obtain top-$k$ semantically relevant documents based on vector similarity.
    \item \textbf{Context Injection:} Retrieved documents are concatenated with the original query and formatted into a prompt.
    \item \textbf{Generation:} The formatted prompt is passed to the language model to generate a context-aware and informative response.
\end{enumerate}

RAG has become a cornerstone for building knowledge-intensive NLP systems, especially in enterprise search, question answering, and summarization tasks \cite{rag_blog, langchain_rag}.

\section{\texttt{llama.cpp}}

\texttt{llama.cpp} is a C++ implementation of LLaMA models developed by Meta, optimized for local inference on commodity hardware without GPU requirements. Built upon the GGML tensor library, it provides quantized inference for large models using CPU-friendly formats like 4-bit and 5-bit quantization, making it suitable for running models such as LLaMA, Mistral, and other open-weight transformers on devices ranging from laptops to Raspberry Pi \cite{llamacpp}.

Key features of \texttt{llama.cpp} include:
\begin{itemize}
    \item Highly efficient CPU inference with quantized models.
    \item Cross-platform support (macOS, Linux, Windows).
    \item Integration with popular tooling such as LangChain and Open Interpreter.
    \item Support for multi-threaded inference and memory-mapped model weights for efficient memory usage.
\end{itemize}

This project is a foundation for many desktop LLM applications that require local execution and privacy-aware computation.

\section{Ollama}

Ollama is a developer-friendly platform for running LLMs locally with simplified model management and serving. It wraps models like LLaMA 2, Mistral, and Code LLaMA into a streamlined runtime with a CLI and RESTful API, abstracting away hardware-specific setup and providing a plug-and-play experience for developers \cite{ollama}.

Ollama supports:
\begin{itemize}
    \item Running quantized models locally with GPU acceleration where available.
    \item Seamless model downloading and serving.
    \item Custom model creation using a simple Modelfile syntax.
\end{itemize}

It is widely used for prototyping private, offline chatbots and assistants.

\section{Llamafile}

Llamafile, developed by Mozilla-Ocho, enables packaging a complete LLM runtime into a single, self-contained executable file \cite{llamafile}. It leverages the \texttt{llama.cpp} backend and Cosmopolitan Libc to build universal binaries that run across major operating systems (Windows, macOS, Linux) without requiring dependencies.

Notable features:
\begin{itemize}
    \item Distributable as a single file under 1GB (depending on model).
    \item Useful for shipping LLM-based tools with zero-install requirements.
    \item Integrates with web frontends for local chatbot deployment.
\end{itemize}

Llamafile simplifies the deployment of on-device LLMs, especially in constrained environments or where installation overhead is a concern.
