
\chapter {Introduction}
\label{ch:Introduction}

%----------------------
\section{Background and Motivation}
\label{sec:BackgroundAndMotivation}
%----------------------

The field of Natural Language Processing (NLP) has undergone a significant transformation with the advent of Large Language Models (LLMs), which are capable of performing complex language understanding and generation tasks. Groundbreaking works such as the Transformer architecture \cite{vaswani2017attention}, BERT \cite{devlin2018bert}, and GPT-family models \cite{brown2020language, openai2023gpt4} have paved the way for highly capable models that support applications such as summarization \cite{liu2019text}, question answering \cite{izacard2021leveraging}, and document understanding \cite{beltagy2020longformer}. These advances have been further systematized in the concept of foundation models \cite{bommasani2021opportunities}, which emphasize the broad applicability and adaptability of pre-trained LLMs.

Despite their success, most widely used LLM applications operate via cloud-based services, which introduce significant limitations when it comes to privacy, data security, and control over computational resources. This is particularly concerning in academic contexts, where students and researchers often deal with sensitive or proprietary content. Recent studies have raised awareness of the risks associated with exposing private data to generative models, including membership inference \cite{mattern2023membership} and data extraction attacks \cite{nasr2023extraction}. Moreover, surveys indicate increasing usage of LLMs in research and education, highlighting both the demand for such tools and the concerns around data governance \cite{deschenes2024survey, hosseini2023chatgpt}.

Simultaneously, the hardware landscape has evolved to enable local deployment of such models. Apple's M1 and M2 chipsets integrate high-performance CPUs, GPUs, and a dedicated Neural Processing Unit (NPU) through the Apple Neural Engine (ANE). These architectures offer a promising platform for efficient, on-device inference of LLMs, provided the models are adapted appropriately to operate under limited memory and compute budgets.

This convergence of high-capability models, growing privacy concerns, and increasingly powerful consumer hardware forms the backdrop for \textit{Project TLDR}—a standalone desktop application for summarization and question answering over a user-specified corpus. The tool is designed to run entirely offline, preserving user privacy while leveraging optimized LLM inference. The project makes use of modern techniques such as quantization \cite{jacob2017quantization} and low-rank adaptation (LoRA) \cite{hu2021lora} to reduce computational overhead and improve deployment feasibility on M1/M2 hardware. Additionally, the use of Retrieval-Augmented Generation (RAG) \cite{lewis2020rag} ensures that answers and summaries are grounded in user-provided text, enhancing both contextual relevance and factual consistency.

In essence, this project is motivated by the goal of empowering academic users with a practical, secure, and efficient means of engaging with large volumes of textual data. By tying together advances in NLP, secure computing practices, and consumer-grade hardware acceleration, Project TLDR aims to demonstrate that high-quality language understanding can be brought directly to the user's device—without compromise.
%----------------------
\section{Our Contributions}
\label{sec:OurContributions}
%----------------------

In this project, we present \textit{Project TLDR}, a privacy-preserving, offline, and resource-efficient desktop application that enables users to perform Question Answering (QA) and Summarization over personal document repositories. Designed primarily for MacOS systems powered by Apple’s M1 and M2 architectures, the application aims to support academic and research workflows where confidentiality, simplicity, and efficiency are paramount.

Our key contributions are as follows:

\begin{itemize}
    
    \item \textbf{Novel Utilization of Apple Neural Engine (ANE)}: A significant technical contribution of this project is our investigation into utilizing Apple’s underused Neural Processing Unit (ANE), capable of up to 11 TOPS in INT8 precision \cite{apple2024ane}. While current LLM deployment frameworks such as LLaMA.cpp \cite{llamacpp2023} or Ollama\cite{ollama2023} do not harness this co-processor, we demonstrate and document methods to tap into the ANE for local inference acceleration. We build on the NPU API reverse-engineering work by tinygrad \cite{tinygrad2023ane} and leverage the learnings to open a new direction for efficient LLM deployment on Apple silicon devices. We hence leverage the NPU outside of traditional CoreML model deployment paradigm and demonstrate how it can be used for various use cases.

    \item \textbf{Retrieval-Augmented Generation (RAG) Architecture}: We implement a lightweight yet effective RAG pipeline \cite{lewis2020rag} for performing QA and summarization tasks over local collections of documents. This enables the application to provide context-grounded, source-aware responses from user-specified corpora while leveraging limited compute resources.

    \item \textbf{Efficient On-Device Inference Using Quantized LLMs}: We leverage quantized transformer models \cite{jacob2017quantization}, reducing memory and compute demands without compromising output quality. Instead of multi-gigabyte model downloads (as required by tools like Ollama \cite{ollama2023} or LLaMA.cpp \cite{llamacpp2023}), we use compact models (50–500MB) that support practical usage scenarios with minimal setup, enhancing portability and usability for non-technical users.

    \item \textbf{User-Friendly and Ready-to-Use Design}: Unlike tools such as Ollama \cite{ollama2023} and LLaMAFile\cite{llamafile2023}, which require technical familiarity and understanding the nuances of various models, our application provides a clean graphical interface with ready-to-use capabilities tailored to common academic needs—eliminating the steep learning curve and reducing operational friction.
    
    \item \textbf{Privacy-Preserving Document Analysis}: By running entirely on-device, our application mitigates the risks associated with uploading sensitive or proprietary documents to third-party services (e.g., ChatGPT, Claude, Gemini), which have raised concerns over data leakage \cite{mattern2023membership,nasr2023extraction}. Users can securely summarize, query, and rephrase information without network access or cloud APIs.

\end{itemize}

Through this suite of contributions, \textit{Project TLDR} demonstrates that meaningful and secure LLM-powered applications can be brought directly to end-users without reliance on cloud services or specialized technical knowledge, thereby filling a critical gap in the current LLM applications ecosystem.


%----------------------
\section{Scope}
\label{sec:Scope}
%----------------------
The proposed solution focuses on the simple case of a fluid flow interacting with a stationary shape in a 2-dimensional environment and replicates the fluid's behavior using a DL model as accurately as possible while improving the execution time compared to a traditional simulation. 

Because the Navier-Stokes equations used in fluid dynamics are so complex and chaotic (See Section~\ref{ch:TheoreticalBackground}), finding an analytical solution for some problems is impossible. This is why numerical techniques are used to approximate the solutions. Research and industry rely on approximated results to perform their experiments and designs. This means that even when the model solution results are not so precise but provide a fast approximation of the data, it still has value since it is a tool to quickly iterate initial designs that can later be validated with more accurate but slow and resource-demanding methods.

%----------------------
\section{Paper overview}
\label{sec:PaperOverview}
%----------------------
This paper is organized as follows: Chapter~\ref{ch:TheoreticalBackground} explains the main concepts for this work related to Computational Fluid Dynamics and Deep Learning. Chapter~\ref{ch:RelatedWork} presents related work and the current state of DL research for CFD and discusses previous related research relevant to this study. Chapter~\ref{ch:Methods} explains all the methods involved in developing this research and the solution, including the data collection, the DL model architecture and training, and the evaluation techniques. Chapter~\ref{ch:Results} shows the results with its analysis and discussion. Finally, Chapter~\ref{ch:Conclusion} presents the conclusions of this research, its limitations, and future work based on the results obtained.

